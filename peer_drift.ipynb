{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IV34zNqCNydU"
      },
      "outputs": [],
      "source": [
        "# Cell 1 - config\n",
        "import pyodbc, math\n",
        "import pandas as pd, numpy as np\n",
        "from scipy import stats\n",
        "from datetime import date, timedelta\n",
        "\n",
        "# === EDIT THESE ===\n",
        "ODBC_DSN = \"DSN=YourHiveDSN;UID=your_user;PWD=your_password\"\n",
        "TXN_TABLE = \"db.txns\"                  # raw transaction table (partitioned by txn_date)\n",
        "DAILY_AGG_TABLE = \"db.cohort_daily_agg\"\n",
        "COHORT_STATS_TABLE = \"db.cohort_stats_for_test\"\n",
        "COHORT_FLAGS_TABLE = \"db.cohort_drift_flags\"\n",
        "# ==================\n",
        "\n",
        "# Parameters\n",
        "RECENT_DAYS = 7\n",
        "BASELINE_DAYS = 90\n",
        "MIN_N_RECENT = 30\n",
        "MIN_N_BASE = 100\n",
        "ALPHA = 0.01\n",
        "MIN_EFFECT_LOG = 0.5   # ~65% increase\n",
        "TODAY = date.today()   # job date - adjust if testing historic runs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 - helpers\n",
        "def get_conn():\n",
        "    return pyodbc.connect(ODBC_DSN, autocommit=True)\n",
        "\n",
        "def run_sql(conn, sql):\n",
        "    \"\"\"Run a SQL statement (DDL/aggregation) via ODBC. Returns nothing.\"\"\"\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(sql)\n",
        "    cur.close()\n",
        "\n",
        "def fetch_df(conn, sql):\n",
        "    \"\"\"Run a SELECT and return pandas DataFrame.\"\"\"\n",
        "    return pd.read_sql(sql, conn)\n"
      ],
      "metadata": {
        "id": "6tHZeafyOY9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 - create/overwrite daily cohort aggregates for the needed window\n",
        "conn = get_conn()\n",
        "\n",
        "start_scan_date = (TODAY - timedelta(days=BASELINE_DAYS + RECENT_DAYS - 1)).isoformat()\n",
        "\n",
        "daily_agg_sql = f\"\"\"\n",
        "-- Compute daily cohort aggregates; adapt cohort definition as needed.\n",
        "INSERT OVERWRITE TABLE {DAILY_AGG_TABLE}\n",
        "PARTITION (txn_date)\n",
        "SELECT\n",
        "  concat(\n",
        "    CASE WHEN days_since_account_opened <= 30 THEN 'new' WHEN days_since_account_opened <= 180 THEN 'mid' ELSE 'old' END,\n",
        "    '_',\n",
        "    CASE WHEN monthly_spend < 500 THEN 'low' WHEN monthly_spend < 2000 THEN 'med' ELSE 'high' END,\n",
        "    '_', channel, '_', merchant_state\n",
        "  ) as cohort_key,\n",
        "  to_date(txn_ts) as txn_date,\n",
        "  count(*) as n,\n",
        "  sum(log(amount + 1)) as sum_log_amt,\n",
        "  sum(log(amount + 1) * log(amount + 1)) as sum_sq_log_amt\n",
        "FROM {TXN_TABLE}\n",
        "WHERE to_date(txn_ts) >= '{start_scan_date}'\n",
        "GROUP BY concat(\n",
        "    CASE WHEN days_since_account_opened <= 30 THEN 'new' WHEN days_since_account_opened <= 180 THEN 'mid' ELSE 'old' END,\n",
        "    '_',\n",
        "    CASE WHEN monthly_spend < 500 THEN 'low' WHEN monthly_spend < 2000 THEN 'med' ELSE 'high' END,\n",
        "    '_', channel, '_', merchant_state\n",
        "  ), to_date(txn_ts)\n",
        ";\n",
        "\"\"\"\n",
        "\n",
        "print(\"Submitting daily aggregate job to Hive (this runs in Hive).\")\n",
        "run_sql(conn, daily_agg_sql)\n",
        "print(\"Daily aggregates written (check table):\", DAILY_AGG_TABLE)\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "FdIQkSsxOcFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 - compute cohort-level aggregated stats in Hive\n",
        "conn = get_conn()\n",
        "\n",
        "recent_start = (TODAY - timedelta(days=RECENT_DAYS-1)).isoformat()\n",
        "recent_end = TODAY.isoformat()\n",
        "baseline_start = (TODAY - timedelta(days=BASELINE_DAYS + RECENT_DAYS -1)).isoformat()\n",
        "baseline_end = (TODAY - timedelta(days=RECENT_DAYS)).isoformat()\n",
        "\n",
        "cohort_stats_sql = f\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS {COHORT_STATS_TABLE} (\n",
        "  cohort_key STRING,\n",
        "  recent_n BIGINT, recent_sum DOUBLE, recent_sum_sq DOUBLE,\n",
        "  base_n BIGINT, base_sum DOUBLE, base_sum_sq DOUBLE,\n",
        "  recent_mean DOUBLE, base_mean DOUBLE, recent_var DOUBLE, base_var DOUBLE\n",
        ") STORED AS PARQUET\n",
        ";\n",
        "\n",
        "INSERT OVERWRITE TABLE {COHORT_STATS_TABLE}\n",
        "SELECT\n",
        "  r.cohort_key,\n",
        "  r.recent_n, r.recent_sum, r.recent_sum_sq,\n",
        "  b.base_n, b.base_sum, b.base_sum_sq,\n",
        "  CASE WHEN r.recent_n > 0 THEN r.recent_sum / r.recent_n ELSE NULL END as recent_mean,\n",
        "  CASE WHEN b.base_n > 0 THEN b.base_sum / b.base_n ELSE NULL END as base_mean,\n",
        "  CASE WHEN r.recent_n > 1 THEN (r.recent_sum_sq - (r.recent_sum*r.recent_sum)/r.recent_n) / (r.recent_n - 1) ELSE NULL END as recent_var,\n",
        "  CASE WHEN b.base_n > 1 THEN (b.base_sum_sq - (b.base_sum*b.base_sum)/b.base_n) / (b.base_n - 1) ELSE NULL END as base_var\n",
        "FROM\n",
        "  ( SELECT cohort_key,\n",
        "           sum(n) as recent_n, sum(sum_log_amt) as recent_sum, sum(sum_sq_log_amt) as recent_sum_sq\n",
        "    FROM {DAILY_AGG_TABLE}\n",
        "    WHERE txn_date >= '{recent_start}' AND txn_date <= '{recent_end}'\n",
        "    GROUP BY cohort_key\n",
        "  ) r\n",
        "JOIN\n",
        "  ( SELECT cohort_key,\n",
        "           sum(n) as base_n, sum(sum_log_amt) as base_sum, sum(sum_sq_log_amt) as base_sum_sq\n",
        "    FROM {DAILY_AGG_TABLE}\n",
        "    WHERE txn_date >= '{baseline_start}' AND txn_date <= '{baseline_end}'\n",
        "    GROUP BY cohort_key\n",
        "  ) b\n",
        "ON r.cohort_key = b.cohort_key\n",
        ";\n",
        "\"\"\"\n",
        "\n",
        "print(\"Running cohort stats job in Hive (small).\")\n",
        "run_sql(conn, cohort_stats_sql)\n",
        "print(\"Cohort stats table written:\", COHORT_STATS_TABLE)\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "cg18ImSBOf0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 - fetch cohort stats to pandas\n",
        "conn = get_conn()\n",
        "sql = f\"SELECT cohort_key, recent_n, recent_mean, recent_var, base_n, base_mean, base_var FROM {COHORT_STATS_TABLE}\"\n",
        "df = fetch_df(conn, sql)\n",
        "print(\"Rows fetched:\", len(df))\n",
        "conn.close()\n",
        "\n",
        "# quick sanity\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "smfoTjFyOnb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 - compute stats and flags\n",
        "def compute_flags(df, alpha=ALPHA, min_effect=MIN_EFFECT_LOG, min_n_recent=MIN_N_RECENT, min_n_base=MIN_N_BASE):\n",
        "    df2 = df.copy()\n",
        "    # filter small counts\n",
        "    df2 = df2[(df2['recent_n'] >= min_n_recent) & (df2['base_n'] >= min_n_base)].reset_index(drop=True)\n",
        "    # safe variance\n",
        "    df2['recent_var'] = df2['recent_var'].replace({0:1e-9}).fillna(1e-9)\n",
        "    df2['base_var']   = df2['base_var'].replace({0:1e-9}).fillna(1e-9)\n",
        "    # t-stat\n",
        "    df2['denom'] = np.sqrt(df2['recent_var']/df2['recent_n'] + df2['base_var']/df2['base_n'])\n",
        "    df2['t_stat'] = (df2['recent_mean'] - df2['base_mean']) / df2['denom']\n",
        "    # Welch df\n",
        "    num = (df2['recent_var']/df2['recent_n'] + df2['base_var']/df2['base_n'])**2\n",
        "    den = ((df2['recent_var']/df2['recent_n'])**2) / (df2['recent_n'] - 1) + ((df2['base_var']/df2['base_n'])**2) / (df2['base_n'] - 1)\n",
        "    df2['df'] = num / den\n",
        "    df2['df'] = df2['df'].replace([np.inf, -np.inf], np.nan).fillna(1.0)\n",
        "    # p-value two-sided\n",
        "    df2['p_value'] = df2.apply(lambda r: 2.0 * stats.t.sf(abs(r['t_stat']), df=r['df']), axis=1)\n",
        "    df2['mean_diff'] = df2['recent_mean'] - df2['base_mean']\n",
        "    df2['drift_flag'] = ((df2['p_value'] < alpha) & (df2['mean_diff'] >= min_effect))\n",
        "    return df2\n",
        "\n",
        "df_flags = compute_flags(df)\n",
        "print(\"Cohorts evaluated:\", len(df), \"Flags:\", df_flags['drift_flag'].sum())\n",
        "df_flags.sort_values('p_value').head(10)\n"
      ],
      "metadata": {
        "id": "PNcoXcsMOp7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 - write small flags back to Hive via batched INSERTs\n",
        "def write_flags_odbc(df_flags, flags_table=COHORT_FLAGS_TABLE, run_date=TODAY.isoformat(), batch=200):\n",
        "    conn = get_conn()\n",
        "    cur = conn.cursor()\n",
        "    # create table if not exists (simple)\n",
        "    create_sql = f\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS {flags_table} (\n",
        "      cohort_key STRING, run_date DATE, recent_n BIGINT, base_n BIGINT,\n",
        "      recent_mean DOUBLE, base_mean DOUBLE, recent_var DOUBLE, base_var DOUBLE,\n",
        "      t_stat DOUBLE, df DOUBLE, p_value DOUBLE, mean_diff DOUBLE, drift_flag BOOLEAN\n",
        "    ) STORED AS PARQUET\n",
        "    \"\"\"\n",
        "    cur.execute(create_sql)\n",
        "    # prepare rows\n",
        "    rows = []\n",
        "    for _, r in df_flags.iterrows():\n",
        "        rows.append((\n",
        "            str(r['cohort_key']).replace(\"'\", \"''\"), run_date,\n",
        "            int(r['recent_n']), int(r['base_n']),\n",
        "            float(r['recent_mean']), float(r['base_mean']),\n",
        "            float(r['recent_var']), float(r['base_var']),\n",
        "            float(r['t_stat']), float(r['df']),\n",
        "            float(r['p_value']), float(r['mean_diff']),\n",
        "            bool(r['drift_flag'])\n",
        "        ))\n",
        "        if len(rows) >= batch:\n",
        "            vals = \",\".join([\"('{}','{}',{},{},{:.12g},{:.12g},{:.12g},{:.12g},{:.12g},{:.12g},{:.12g},{:.12g},{})\".format(*row) for row in rows])\n",
        "            sql = f\"INSERT INTO TABLE {flags_table} VALUES {vals}\"\n",
        "            cur.execute(sql)\n",
        "            rows = []\n",
        "    if rows:\n",
        "        vals = \",\".join([\"('{}','{}',{},{},{:.12g},{:.12g},{:.12g},{:.12g},{:.12g},{:.12g},{:.12g},{:.12g},{})\".format(*row) for row in rows])\n",
        "        sql = f\"INSERT INTO TABLE {flags_table} VALUES {vals}\"\n",
        "        cur.execute(sql)\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    print(\"Wrote flags to Hive table:\", flags_table)\n",
        "\n",
        "# call it (writes all cohorts back; but only a few rows per day will have drift_flag=True)\n",
        "write_flags_odbc(df_flags)\n"
      ],
      "metadata": {
        "id": "ArRC33H4Orkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 - quick backtest SQL (runs inside Hive). Edit FRAUD_TXN_TABLE to your fraud-labeled txn table.\n",
        "FRAUD_TXN_TABLE = \"db.txns\"   # if your txns contain fraud_flag column use this; otherwise adjust\n",
        "\n",
        "W = 14  # lookahead days\n",
        "# Build a Hive query that finds, for each flagged cohort on run_date, whether any fraud txn occurs in next W days.\n",
        "backtest_sql = f\"\"\"\n",
        "SELECT f.cohort_key, f.run_date, f.drift_flag,\n",
        "       SUM(CASE WHEN t.fraud_flag = 1 AND to_date(t.txn_ts) BETWEEN date_add(f.run_date, 0) AND date_add(f.run_date, {W}) THEN 1 ELSE 0 END) as fraud_count_in_window\n",
        "FROM {COHORT_FLAGS_TABLE} f\n",
        "LEFT JOIN {FRAUD_TXN_TABLE} t\n",
        "  ON concat(\n",
        "        CASE WHEN t.days_since_account_opened <= 30 THEN 'new' WHEN t.days_since_account_opened <= 180 THEN 'mid' ELSE 'old' END,\n",
        "        '_',\n",
        "        CASE WHEN t.monthly_spend < 500 THEN 'low' WHEN t.monthly_spend < 2000 THEN 'med' ELSE 'high' END,\n",
        "        '_', t.channel, '_', t.merchant_state\n",
        "     ) = f.cohort_key\n",
        "GROUP BY f.cohort_key, f.run_date, f.drift_flag\n",
        "HAVING f.run_date = '{TODAY.isoformat()}'\n",
        ";\n",
        "\"\"\"\n",
        "\n",
        "print(\"Submitting backtest SQL to Hive; returns a cohort-level fraud count in the lookahead window.\")\n",
        "conn = get_conn()\n",
        "df_backtest = fetch_df(conn, backtest_sql)\n",
        "conn.close()\n",
        "print(df_backtest.head())\n",
        "print(\"Flagged cohorts with any fraud in lookahead:\", (df_backtest['fraud_count_in_window']>0).sum(), \"out of\", len(df_backtest))\n"
      ],
      "metadata": {
        "id": "lb5vc9SVOuS8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}