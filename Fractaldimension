import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from scipy.stats import linregress

# Combined signal generation with rolling lookback
def build_fraud_signals_with_lookback(df, lookback_days=30, box_sizes=[0.1, 0.05, 0.01]):
    df['transferdate'] = pd.to_datetime(df['transferdate'])
    results = []

    for party_id, group in df.groupby('partyid'):
        max_date = group['transferdate'].max()
        cutoff = max_date - pd.Timedelta(days=lookback_days)
        recent_txns = group[group['transferdate'] >= cutoff].sort_values('transferdate')

        if len(recent_txns) < 3:
            continue  # Skip if not enough data

        recent_txns['time_gap'] = recent_txns['transferdate'].diff().dt.total_seconds().fillna(0)
        recent_txns['log_amount'] = np.log1p(recent_txns['transfer_amount'])

        scaler = MinMaxScaler()
        normalized = scaler.fit_transform(recent_txns[['time_gap', 'log_amount']])
        recent_txns[['x', 'y']] = normalized

        box_counts = []
        for box_size in box_sizes:
            boxes = set()
            for _, row in recent_txns.iterrows():
                ix = int(np.floor(row['x'] / box_size))
                iy = int(np.floor(row['y'] / box_size))
                boxes.add((ix, iy))
            box_counts.append(len(boxes))

        log_box_sizes = np.log(1 / np.array(box_sizes))
        log_counts = np.log(box_counts)
        slope, _, _, _, _ = linregress(log_box_sizes, log_counts)
        fractal_dimension = slope

        avg_gap = recent_txns['time_gap'].mean()
        bank_div = recent_txns['bankname'].nunique()
        utilization = (recent_txns['transfer_amount'] / recent_txns['credit_limit']).mean()
        variability = recent_txns['transfer_amount'].std()

        results.append({
            'partyid': party_id,
            'fractal_dimension': fractal_dimension,
            'avg_time_gap': avg_gap,
            'bank_diversity': bank_div,
            'limit_utilization_mean': utilization,
            'transfer_variability': variability,
            'lookback_cutoff': cutoff
        })

    return pd.DataFrame(results)

# Final scoring and flagging
def flag_risky_transactions_dynamic(df, lookback_days=30):
    signals = build_fraud_signals_with_lookback(df, lookback_days)
    df = df.merge(signals, on='partyid', how='left')

    # Risk logic
    df['reason_fractal'] = df['fractal_dimension'] > 1.8
    df['reason_gap'] = df['avg_time_gap'] < 300
    df['reason_bankdiv'] = df['bank_diversity'] > 5
    df['reason_utilization'] = df['limit_utilization_mean'] > 0.8
    df['reason_variability'] = df['transfer_variability'] > 10000

    df['risk_score'] = (
        df['reason_fractal'].astype(int) * 2 +
        df['reason_gap'].astype(int) * 1 +
        df['reason_bankdiv'].astype(int) * 2 +
        df['reason_utilization'].astype(int) * 1 +
        df['reason_variability'].astype(int) * 1
    )

    df['risk_flag'] = (
        (df['risk_score'] >= 3) &
        (df['transferdate'] >= df['lookback_cutoff'])
    ).astype(int)

    def reason_explainer(row):
        reasons = []
        if row['reason_fractal']: reasons.append('High fractal dimension')
        if row['reason_gap']: reasons.append('Short time gaps')
        if row['reason_bankdiv']: reasons.append('Many banks used')
        if row['reason_utilization']: reasons.append('High limit utilization')
        if row['reason_variability']: reasons.append('High transfer variability')
        return '; '.join(reasons)

    df['risk_reason'] = df.apply(reason_explainer, axis=1)
    return df[['partyid', 'transferdate', 'transfer_amount', 'risk_score', 'risk_flag', 'risk_reason']]
